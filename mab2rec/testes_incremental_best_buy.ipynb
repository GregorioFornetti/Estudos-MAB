{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"/workspace/gregorio/reinforcement-learning-recsys/1-datasets/bestbuy/interactions.csv\", sep=';')\n",
    "    df = df.rename(columns={\n",
    "        'id_user': 'user_id',\n",
    "        'id_item': 'item_id',\n",
    "    })\n",
    "    df['response'] = 1\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    df = df[['user_id', 'item_id', 'response']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534174</th>\n",
       "      <td>496cde27a7d6a3d4989c8a7143f7a7573dcad18e</td>\n",
       "      <td>1658122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158250</th>\n",
       "      <td>9efe144125a01b1ed7301e9cba939a3f3f33ef13</td>\n",
       "      <td>2969477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458890</th>\n",
       "      <td>c83a96b80b0fb276a93ec8b5c3cc9df57f53914d</td>\n",
       "      <td>999996500050001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433387</th>\n",
       "      <td>3ba3fb6612eb4f198673be714dde48d14d2f9d3c</td>\n",
       "      <td>9999161700050000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234540</th>\n",
       "      <td>2059e46ae923f4227600e8edb620ae898bd30d7d</td>\n",
       "      <td>1283795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265215</th>\n",
       "      <td>2489027d9f516d21ea7a08874361a23afb4efb5c</td>\n",
       "      <td>3504885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880164</th>\n",
       "      <td>7911c95c0a36383d15f5c440077e4bd12927b61c</td>\n",
       "      <td>1722057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710946</th>\n",
       "      <td>61c0b8b1c8bb7d13769f65cfe3827c93faa899e7</td>\n",
       "      <td>1177301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394558</th>\n",
       "      <td>bf6f173158e042f8dc9f6ebf7eeed6690b992f5c</td>\n",
       "      <td>3011735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087519</th>\n",
       "      <td>954f46064ad79ee26eda49597f49f4a3c1d9a125</td>\n",
       "      <td>3158734</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1865269 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_id           item_id  response\n",
       "534174   496cde27a7d6a3d4989c8a7143f7a7573dcad18e           1658122         1\n",
       "1158250  9efe144125a01b1ed7301e9cba939a3f3f33ef13           2969477         1\n",
       "1458890  c83a96b80b0fb276a93ec8b5c3cc9df57f53914d   999996500050001         1\n",
       "433387   3ba3fb6612eb4f198673be714dde48d14d2f9d3c  9999161700050000         1\n",
       "234540   2059e46ae923f4227600e8edb620ae898bd30d7d           1283795         1\n",
       "...                                           ...               ...       ...\n",
       "265215   2489027d9f516d21ea7a08874361a23afb4efb5c           3504885         1\n",
       "880164   7911c95c0a36383d15f5c440077e4bd12927b61c           1722057         1\n",
       "710946   61c0b8b1c8bb7d13769f65cfe3827c93faa899e7           1177301         1\n",
       "1394558  bf6f173158e042f8dc9f6ebf7eeed6690b992f5c           3011735         1\n",
       "1087519  954f46064ad79ee26eda49597f49f4a3c1d9a125           3158734         1\n",
       "\n",
       "[1865269 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando os modelos MAB usando concatenações de diferentes formas de fazer o contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gregorio/.conda/envs/weighted-sims/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gregorio/.conda/envs/weighted-sims/lib/python3.8/site-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: no CUDA-capable device is detected (/project/./implicit/gpu/utils.h:71)'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "import plotly.express as px\n",
    "import time\n",
    "import os\n",
    "import implicit\n",
    "from mab2rec import BanditRecommender, LearningPolicy\n",
    "\n",
    "train_data = \"../data/ml100k/data_train.csv\"\n",
    "test_data = \"../data/ml100k/data_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTORS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_embeddings_model(Model, df, num_users, num_items, generate_embeddings=False):\n",
    "    sparse_matrix = csr_matrix((df['response'], (df['user_id'], df['item_id'])), shape=(num_users, num_items))\n",
    "\n",
    "    model = Model(factors=FACTORS, random_state=1)\n",
    "    model.fit(sparse_matrix)\n",
    "\n",
    "    if not generate_embeddings:\n",
    "        return model, sparse_matrix\n",
    "    \n",
    "    # # Não precisamos mais do código abaixo, ele funcina para embeddings de usuário, não de itens\n",
    "    # user_features_list = []\n",
    "\n",
    "    # for user_id in df['user_id'].unique():\n",
    "    #    user_factors = model.user_factors[user_id][:FACTORS]  # O BPR coloca 1 no final dos vetores latentes ?\n",
    "    #    user_features_list.append([user_id] + list(user_factors))\n",
    "\n",
    "    # df_user_features = pd.DataFrame(user_features_list, columns=['user_id'] + [f'u{i}' for i in range(FACTORS)])\n",
    "\n",
    "    # model = model.to_cpu()\n",
    "    return model, sparse_matrix, model.item_factors, model.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_embeddings_model(model, sparse_matrix, df_test):\n",
    "    all_recs = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    hits = 0\n",
    "    for _, interaction in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        ids_recs, _ = model.recommend(userid=interaction['user_id'], user_items=sparse_matrix[interaction['user_id']], N=10)\n",
    "        if interaction['item_id'] in ids_recs:\n",
    "            hits += 1\n",
    "        all_recs.append(ids_recs.tolist())\n",
    "    \n",
    "    recs_df = pd.DataFrame({\n",
    "        'interaction_number': [i for i in range(len(df_test))],\n",
    "        'user_id': df_test['user_id'],\n",
    "        'item_id': df_test['item_id'],\n",
    "        'recommendations': all_recs\n",
    "    })\n",
    "    \n",
    "    return hits, hits/len(df_test), time.time() - start_time, recs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_mab(mab_algo, df_train_with_contexts, contexts_col):\n",
    "    contexts = get_concat_context(df_train_with_contexts, contexts_col)\n",
    "    mab_algo.fit(\n",
    "        decisions=df_train_with_contexts['item_id'],\n",
    "        rewards=df_train_with_contexts['response'],\n",
    "        contexts=contexts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_non_incremental(mab_algo, contexts_col, df_test, interactions_by_user):\n",
    "    start_time = time.time()\n",
    "    hits = 0\n",
    "\n",
    "    # contexts = df_test.merge(user_features, how='left', on='user_id').drop(columns=['user_id', 'item_id', 'response']).values\n",
    "    # contexts = np.array(df_test[contexts_col].tolist())\n",
    "    print('entrou')\n",
    "    contexts = get_concat_context(df_test, contexts_col)\n",
    "    filters = df_test.merge(interactions_by_user, how='left', on='user_id')[['interactions']].values.squeeze(axis=1) \n",
    "    print('saiu')\n",
    "\n",
    "    recomendations = mab_algo.recommend(contexts, filters, apply_sigmoid=False)\n",
    "\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    hits = 0\n",
    "    for i, interaction in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        if interaction['item_id'] in recomendations[i]:\n",
    "            hits += 1\n",
    "    \n",
    "\n",
    "    recs_df = pd.DataFrame({\n",
    "        'interaction_number': [i for i in range(len(df_test))],\n",
    "        'user_id': df_test['user_id'],\n",
    "        'item_id': df_test['item_id'],\n",
    "        'recommendations': recomendations\n",
    "    })\n",
    "\n",
    "    return hits, hits/len(df_test), time.time() - start_time, recs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_interactions_by_user(interactions_df):\n",
    "    interactions_by_user = interactions_df\\\n",
    "                        .groupby('user_id')[['item_id']]\\\n",
    "                        .apply(lambda df_user: df_user['item_id'].tolist())\\\n",
    "                        .reset_index(name='interactions')\n",
    "    interactions_by_user = interactions_by_user.reset_index(drop=True)\n",
    "    return interactions_by_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_list_items_mean(interactions_df, items_embeddings):\n",
    "    users_current_info = {}\n",
    "    contexts = []\n",
    "\n",
    "    for _, row in tqdm(interactions_df.iterrows(), total=len(interactions_df)):\n",
    "        user_id = row[\"user_id\"]\n",
    "        item_id = row[\"item_id\"]\n",
    "\n",
    "        if user_id not in users_current_info:\n",
    "            users_current_info[user_id] = {\n",
    "                'acum_emb': np.zeros((FACTORS, )),\n",
    "                'count': 0\n",
    "            }\n",
    "        \n",
    "        contexts.append(users_current_info[user_id]['acum_emb'] / max(1, users_current_info[user_id]['count']))\n",
    "\n",
    "        users_current_info[user_id]['acum_emb'] += items_embeddings[item_id][:FACTORS]\n",
    "        users_current_info[user_id]['count'] += 1\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_list_items_concat(interactions_df, items_embeddings, window_size):\n",
    "    users_current_info = {}\n",
    "    contexts = []\n",
    "\n",
    "    for _, row in interactions_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        item_id = row[\"item_id\"]\n",
    "\n",
    "        if user_id not in users_current_info:\n",
    "            users_current_info[user_id] = np.zeros((window_size, FACTORS))\n",
    "        \n",
    "        contexts.append(users_current_info[user_id].flatten())\n",
    "        \n",
    "        users_current_info[user_id][1:] = users_current_info[user_id][:-1]\n",
    "        users_current_info[user_id][0] = items_embeddings[item_id][:FACTORS]\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_list_user(interactions_df, users_embeddings):\n",
    "    contexts = []\n",
    "\n",
    "    for _, row in interactions_df.iterrows():\n",
    "        user_id = row[\"user_id\"]\n",
    "        contexts.append(users_embeddings[user_id][:FACTORS])\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_context(interactions, context_cols):\n",
    "    # Concat multiple array columns into a single array column\n",
    "    return np.array(interactions[context_cols].apply(lambda x: np.concatenate((*x, [1])), axis=1).tolist())  # MUDANÇA: adiciona 1 ao final de cada vetor (bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def test(test_size, train_initial_size, train_extra_increment_step_size, windows_sizes):\n",
    "    '''\n",
    "    - `test_size`: define o tamanho da partição de teste no train/test split inicial. Por exemplo, se for escolhido 0.1 (10%), a partição de teste terá 10% das interações e a partição de treino terá 90% das interações. O tamanho da partição de teste passará ainda por um filtro com o tamanho do treino inicial, definido no próximo parâmetro.\n",
    "    - `train_initial_size`: define o tamanho inicial que será usado para treino dos modelos. Esse tamanho é uma porcentagem da partição de treino, por exemplo, 0.5 (50%) quer dizer que o treino será feito inicialmente com 50% das interações separadas para treino. Vale ressaltar que essa porcentagem é relacionada apenas à partição de treino, então, se temos uma partição de treino de 0.9 (90%) e o “train_initial_size” é definido como 0.5 (50%), então, teremos 45% (0.9 * 0.5) das interações todas para o treino inicial. Com a base de treino separada com essa porcentagem inicial, a base de teste passara por um filtro, removendo todas as interações com itens ou usuários que nunca foram vistos nesse treino inicial.\n",
    "    - `train_extra_increment_step_size`: define a porcentagem do \"treinamento extra\" que será usado. No início a base de dados é separada em treino inicial (train_initial_size), \"treinamento extra\" e teste. O \"treinamento extra\", assim como o teste, passa por um filtro para remover interações com itens ou usuários que nunca foram vistos no treino inicial. Após o treino inicial, o \"treinamento extra\" é usado para treinar os modelos de embeddings e os modelos de bandit. O \"treinamento extra\" é incrementado a cada iteração, de acordo com o valor desse parâmetro. Por exemplo, se o `train_extra_increment_step_size` é 0.1 (10%), então, a cada iteração, 10% das interações são adicionadas ao treino, até que todo o \"treinamento extra\" seja usado.\n",
    "    - `windows_sizes`: tamanho das janelas de contextos que serão usadas para teste. Por exemplo, se for passado [3, 5, 7], as janelas de tamanho de 3, 5 e 7 serão usadas como contexto para treinar os modelos de MAB (gerando resultados diferentes para cada tamanho de janela).\n",
    "    '''\n",
    "    results = []\n",
    "    df_recs = pd.DataFrame(columns=['algorithm', 'interaction_number', 'user_id', 'item_id', 'recommendations'])\n",
    "    # df_train = pd.read_csv(train_data)\n",
    "    # df_test = pd.read_csv(test_data)\n",
    "\n",
    "    df_full = load_data()\n",
    "\n",
    "    df_full['user_id'] = LabelEncoder().fit_transform(df_full['user_id'])\n",
    "    df_full['item_id'] = LabelEncoder().fit_transform(df_full['item_id'])\n",
    "\n",
    "    num_users = df_full['user_id'].nunique()\n",
    "    num_items = df_full['item_id'].nunique()\n",
    "\n",
    "    split_index = int(len(df_full) * (1 - test_size))\n",
    "    df_train_full = df_full[:split_index]\n",
    "    df_test = df_full[split_index:]\n",
    "\n",
    "    initial_df_train = df_train_full[:int(len(df_train_full) * train_initial_size)]\n",
    "    extra_df_train = df_train_full[int(len(df_train_full) * train_initial_size):]\n",
    "    extra_df_train = extra_df_train[(extra_df_train['user_id'].isin(initial_df_train['user_id'])) & (extra_df_train['item_id'].isin(initial_df_train['item_id']))]\n",
    "    extra_df_train = extra_df_train.reset_index(drop=True)\n",
    "\n",
    "    df_test = df_test[(df_test['user_id'].isin(initial_df_train['user_id'])) & (df_test['item_id'].isin(initial_df_train['item_id']))]\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_test_for_evaluation = df_test[df_test['response'] == 1]\n",
    "    df_test_for_evaluation = df_test_for_evaluation.reset_index(drop=True)\n",
    "\n",
    "    print('Generating ALS embeddings')\n",
    "    ALS_model, _, ALS_item_embeddings, ALS_user_embeddings = train_embeddings_model(implicit.als.AlternatingLeastSquares, initial_df_train, num_users, num_items, generate_embeddings=True)\n",
    "\n",
    "    print('Generating BPR embeddings')\n",
    "    BPR_model, _, BPR_item_embeddings, BPR_user_embeddings = train_embeddings_model(implicit.bpr.BayesianPersonalizedRanking, initial_df_train, num_users, num_items, generate_embeddings=True)\n",
    "\n",
    "    '''\n",
    "    for window_size in windows_sizes:\n",
    "        print(f'Generating contexts for window size of {window_size} (contat items emb)')\n",
    "        df_full_new = pd.concat([initial_df_train, extra_df_train, df_test_for_evaluation])\n",
    "        als_contexts = create_contexts_list_items_concat(df_full_new, ALS_item_embeddings, window_size)\n",
    "        bpr_contexts = create_contexts_list_items_concat(df_full_new, BPR_item_embeddings, window_size)\n",
    "\n",
    "        initial_df_train[f'als_context_item_concat_{window_size}'] = als_contexts[:len(initial_df_train)]\n",
    "        initial_df_train[f'bpr_context_item_concat_{window_size}'] = bpr_contexts[:len(initial_df_train)]\n",
    "\n",
    "        extra_df_train[f'als_context_item_concat_{window_size}'] = als_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "        extra_df_train[f'bpr_context_item_concat_{window_size}'] = bpr_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "\n",
    "        df_test_for_evaluation[f'als_context_item_concat_{window_size}'] = als_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "        df_test_for_evaluation[f'bpr_context_item_concat_{window_size}'] = bpr_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "\n",
    "    print('Generating contexts for user embeddings')\n",
    "    df_full_new = pd.concat([initial_df_train, extra_df_train, df_test_for_evaluation])\n",
    "    als_contexts = create_contexts_list_user(df_full_new, ALS_user_embeddings)\n",
    "    bpr_contexts = create_contexts_list_user(df_full_new, BPR_user_embeddings)\n",
    "\n",
    "    initial_df_train['als_context_user'] = als_contexts[:len(initial_df_train)]\n",
    "    initial_df_train['bpr_context_user'] = bpr_contexts[:len(initial_df_train)]\n",
    "\n",
    "    extra_df_train['als_context_user'] = als_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "    extra_df_train['bpr_context_user'] = bpr_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "\n",
    "    df_test_for_evaluation['als_context_user'] = als_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "    df_test_for_evaluation['bpr_context_user'] = bpr_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "    '''\n",
    "    \n",
    "    print('Generating contexts for item mean embeddings')\n",
    "    df_full_new = pd.concat([initial_df_train, extra_df_train, df_test_for_evaluation])\n",
    "    als_contexts = create_contexts_list_items_mean(df_full_new, ALS_item_embeddings)\n",
    "    bpr_contexts = create_contexts_list_items_mean(df_full_new, BPR_item_embeddings)\n",
    "\n",
    "    initial_df_train['als_context_items_mean'] = als_contexts[:len(initial_df_train)]\n",
    "    initial_df_train['bpr_context_items_mean'] = bpr_contexts[:len(initial_df_train)]\n",
    "\n",
    "    extra_df_train['als_context_items_mean'] = als_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "    extra_df_train['bpr_context_items_mean'] = bpr_contexts[len(initial_df_train):len(initial_df_train) + len(extra_df_train)]\n",
    "\n",
    "    df_test_for_evaluation['als_context_items_mean'] = als_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "    df_test_for_evaluation['bpr_context_items_mean'] = bpr_contexts[len(initial_df_train) + len(extra_df_train):]\n",
    "\n",
    "    algos_dict = {\n",
    "        # 'item_concat': {\n",
    "        #     'item_concat': True,\n",
    "        #     'item_mean': False,\n",
    "        #     'user': False\n",
    "        # },\n",
    "        'item_mean': {\n",
    "            'item_concat': False,\n",
    "            'item_mean': True,\n",
    "            'user': False\n",
    "        },\n",
    "        # 'user': {\n",
    "        #     'item_concat': False,\n",
    "        #     'item_mean': False,\n",
    "        #     'user': True\n",
    "        # },\n",
    "        # 'item_concat-item_mean': {\n",
    "        #     'item_concat': True,\n",
    "        #     'item_mean': True,\n",
    "        #     'user': False\n",
    "        # },\n",
    "        # 'item_concat-user': {\n",
    "        #     'item_concat': True,\n",
    "        #     'item_mean': False,\n",
    "        #     'user': True\n",
    "        # },\n",
    "        # 'item_mean-user': {\n",
    "        #     'item_concat': False,\n",
    "        #     'item_mean': True,\n",
    "        #     'user': True\n",
    "        # },\n",
    "        # 'all': {\n",
    "        #     'item_concat': True,\n",
    "        #     'item_mean': True,\n",
    "        #     'user': True\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    for algo_name, _ in algos_dict.items():\n",
    "        algos_dict[algo_name]['results'] = []\n",
    "        algos_dict[algo_name]['df_recs'] = pd.DataFrame(columns=['algorithm', 'interaction_number', 'user_id', 'item_id', 'recommendations'])\n",
    "\n",
    "    def save_algo_result(algo_name, hits, hr, spent_time, df_recs_algo, current_extra_train_size, results):\n",
    "        df_recs_algo['algorithm'] = algo_name\n",
    "        df_recs_algo['train_size'] = current_extra_train_size\n",
    "        df_recs_new = pd.concat([df_recs, df_recs_algo])\n",
    "        results.append({'algorithm': algo_name, 'hits': hits, 'hr': hr, 'time': spent_time, 'train_size': current_extra_train_size})\n",
    "        return df_recs_new\n",
    "\n",
    "    current_extra_train_size = 0\n",
    "    while current_extra_train_size <= 1:\n",
    "        print(f\"Current extra train size: {current_extra_train_size}\")\n",
    "\n",
    "        current_df_train = pd.concat([initial_df_train, extra_df_train[:int(len(extra_df_train) * current_extra_train_size)]])\n",
    "        interactions_by_user = group_interactions_by_user(current_df_train)  # MUDANÇA AQUI\n",
    "\n",
    "        # -------------- ALS -----------------\n",
    "        print('Training ALS')\n",
    "        ALS_model, sparse_matrix = train_embeddings_model(implicit.als.AlternatingLeastSquares, current_df_train, num_users, num_items)\n",
    "\n",
    "        print('Testing ALS')\n",
    "        hits, hr, spent_time, df_recs_als = test_embeddings_model(ALS_model, sparse_matrix, df_test_for_evaluation)\n",
    "        df_recs = save_algo_result('ALS', hits, hr, spent_time, df_recs_als, current_extra_train_size, results)\n",
    "\n",
    "        # -------------- BPR -----------------\n",
    "        print('Training BPR')\n",
    "        BPR_model, sparse_matrix = train_embeddings_model(implicit.bpr.BayesianPersonalizedRanking, current_df_train, num_users, num_items)\n",
    "\n",
    "        print('Testing BPR')\n",
    "        hits, hr, spent_time, df_recs_bpr = test_embeddings_model(BPR_model, sparse_matrix, df_test_for_evaluation)\n",
    "        df_recs = save_algo_result('BPR', hits, hr, spent_time, df_recs_bpr, current_extra_train_size, results)\n",
    "        \n",
    "        for algo_name, algo_dict in algos_dict.items():\n",
    "            if algo_dict['item_concat']:\n",
    "                windows = windows_sizes\n",
    "            else:\n",
    "                windows = [None]\n",
    "            \n",
    "            for window_size in windows:\n",
    "                als_embeddings_cols = []\n",
    "                bpr_embeddings_cols = []\n",
    "                print_extra = f' - {algo_name}'\n",
    "                algo_name_extra = ''\n",
    "                if algo_dict['item_concat']:\n",
    "                    als_embeddings_cols.append(f'als_context_item_concat_{window_size}')\n",
    "                    bpr_embeddings_cols.append(f'bpr_context_item_concat_{window_size}')\n",
    "                    print_extra = f' - {algo_name} - {window_size}'\n",
    "                    algo_name_extra = f' - {window_size}'\n",
    "                if algo_dict['item_mean']:\n",
    "                    als_embeddings_cols.append('als_context_items_mean')\n",
    "                    bpr_embeddings_cols.append('bpr_context_items_mean')\n",
    "                if algo_dict['user']:\n",
    "                    als_embeddings_cols.append('als_context_user')\n",
    "                    bpr_embeddings_cols.append('bpr_context_user')\n",
    "                \n",
    "                # ------ LinUCB - ALS embeddings -------\n",
    "                print(f'Training LinUCB - ALS embeddings{print_extra}')\n",
    "                linUCB_model = BanditRecommender(learning_policy=LearningPolicy.LinUCB(alpha=0.1), top_k=10)\n",
    "                start_time = time.time()\n",
    "                train_mab(linUCB_model, current_df_train, als_embeddings_cols)  # Mudança no treinamento dos MAB\n",
    "                print(f'Treinamento demorou {time.time() - start_time} segundos')\n",
    "\n",
    "                print(f'Testing LinUCB - ALS embeddings{print_extra}')\n",
    "                hits, hr, spent_time, df_recs_linUCB = test_non_incremental(linUCB_model, als_embeddings_cols, df_test_for_evaluation, interactions_by_user)\n",
    "                algo_dict['df_recs'] = save_algo_result(f'LinUCB - ALS embeddings{algo_name_extra}', hits, hr, spent_time, df_recs_linUCB, current_extra_train_size, algo_dict['results'])\n",
    "\n",
    "\n",
    "                # ------ LinUCB - BPR embeddings -------\n",
    "                print(f'Training LinUCB - BPR embeddings{print_extra}')\n",
    "                linUCB_model = BanditRecommender(learning_policy=LearningPolicy.LinUCB(alpha=0.1), top_k=10)\n",
    "                train_mab(linUCB_model, current_df_train, bpr_embeddings_cols)\n",
    "\n",
    "                print(f'Testing LinUCB - BPR embeddings{print_extra}')\n",
    "                hits, hr, spent_time, df_recs_linUCB = test_non_incremental(linUCB_model, bpr_embeddings_cols, df_test_for_evaluation, interactions_by_user)\n",
    "                algo_dict['df_recs'] = save_algo_result(f'LinUCB - BPR embeddings{algo_name_extra}', hits, hr, spent_time, df_recs_linUCB, current_extra_train_size, algo_dict['results'])\n",
    "\n",
    "                # ------ LinGreedy - ALS embeddings -------\n",
    "                print(f'Training LinGreedy - ALS embeddings{print_extra}')\n",
    "                linGreedy_model = BanditRecommender(learning_policy=LearningPolicy.LinGreedy(epsilon=0.01), top_k=10)\n",
    "                train_mab(linGreedy_model, current_df_train, als_embeddings_cols)\n",
    "\n",
    "                print(f'Testing LinGreedy - ALS embeddings{print_extra}')\n",
    "                hits, hr, spent_time, df_recs_linGreedy = test_non_incremental(linGreedy_model, als_embeddings_cols, df_test_for_evaluation, interactions_by_user)\n",
    "                algo_dict['df_recs'] = save_algo_result(f'LinGreedy - ALS embeddings{algo_name_extra}', hits, hr, spent_time, df_recs_linGreedy, current_extra_train_size, algo_dict['results'])\n",
    "\n",
    "\n",
    "                # ------ LinGreedy - BPR embeddings -------\n",
    "                print(f'Training LinGreedy - BPR embeddings{print_extra}')\n",
    "                linGreedy_model = BanditRecommender(learning_policy=LearningPolicy.LinGreedy(epsilon=0.01), top_k=10)\n",
    "                train_mab(linGreedy_model, current_df_train, bpr_embeddings_cols)\n",
    "\n",
    "                print(f'Testing LinGreedy - BPR embeddings{print_extra}')\n",
    "                hits, hr, spent_time, df_recs_linGreedy = test_non_incremental(linGreedy_model, bpr_embeddings_cols, df_test_for_evaluation, interactions_by_user)\n",
    "                algo_dict['df_recs'] = save_algo_result(f'LinGreedy - BPR embeddings{algo_name_extra}', hits, hr, spent_time, df_recs_linGreedy, current_extra_train_size, algo_dict['results'])\n",
    "        \n",
    "        # Incrementando o tamanho do treino para próxima iteração\n",
    "        current_extra_train_size = round(current_extra_train_size + train_extra_increment_step_size, 2)\n",
    "    \n",
    "    save_path = f'results-v14/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    df_results_als_bpr = pd.DataFrame(results)\n",
    "    for algo_name, algo_dict in algos_dict.items():\n",
    "        df_results_final = pd.DataFrame(algo_dict['results'])\n",
    "        df_results_final = pd.concat([df_results_final, df_results_als_bpr])\n",
    "        df_results_final = df_results_final.astype({'hits': int, 'hr': float, 'time': float})\n",
    "        df_results_final['test_size'] = round(test_size, 2)\n",
    "        df_results_final['test_interactions'] = len(df_test_for_evaluation)\n",
    "\n",
    "        df_results_final.to_csv(f'{save_path}/results-{algo_name}.csv', index=False)\n",
    "\n",
    "        df_recs_final = pd.concat([df_recs, algo_dict['df_recs']])\n",
    "        df_recs_final.to_csv(f'{save_path}/recs-{algo_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ALS embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:08<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BPR embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.11it/s, train_auc=94.34%, skipped=0.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating contexts for item mean embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 988247/988247 [01:09<00:00, 14278.66it/s]\n",
      "100%|██████████| 988247/988247 [01:08<00:00, 14375.29it/s]\n",
      "/tmp/ipykernel_5653/4010160064.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  initial_df_train['als_context_items_mean'] = als_contexts[:len(initial_df_train)]\n",
      "/tmp/ipykernel_5653/4010160064.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  initial_df_train['bpr_context_items_mean'] = bpr_contexts[:len(initial_df_train)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current extra train size: 0\n",
      "Training ALS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:06<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ALS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25606/25606 [00:31<00:00, 818.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.35it/s, train_auc=94.36%, skipped=0.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BPR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25606/25606 [00:31<00:00, 822.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinUCB - ALS embeddings - item_mean\n",
      "Treinamento demorou 76.19209218025208 segundos\n",
      "Testing LinUCB - ALS embeddings - item_mean\n",
      "entrou\n",
      "saiu\n",
      "oi1\n",
      "predict_expectations demorou 354.2990942001343 segundos\n",
      "oi2\n"
     ]
    }
   ],
   "source": [
    "test(test_size=0.1, train_initial_size=0.5, train_extra_increment_step_size=0.1, windows_sizes=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('results-v14/results-item_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_results, x=\"train_size\", y=\"hr\", color='algorithm', title='HR x Train size')\n",
    "fig.show()\n",
    "fig.write_html('results-v14/hr_x_train_size.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results[(df_results['algorithm'].str.contains('2')) | (df_results['algorithm'] == 'ALS') | (df_results['algorithm'] == 'BPR')]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['algorithm'] = df_results['algorithm'].str.replace(' - 2', '')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_incremental(df_results):\n",
    "    new_df = df_results[(~df_results['algorithm'].str.contains('incremental') | df_results['algorithm'].str.contains('non-incremental'))]\n",
    "    new_df['algorithm'] = new_df['algorithm'].str.replace(' - non-incremental', '')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_in_upper_and_lower_bounds(df_results, algo_names):\n",
    "    train_sizes = sorted(df_results['train_size'].unique().tolist())\n",
    "    min_train_size = train_sizes[0]\n",
    "    max_train_size = train_sizes[-1]\n",
    "    qnt_train_sizes = len(train_sizes)\n",
    "\n",
    "    for algo_name in algo_names:\n",
    "        algo_row_lower = df_results[(df_results['algorithm'] == algo_name) & (df_results['train_size'] == min_train_size)]\n",
    "        df_lower = pd.DataFrame({\n",
    "            'algorithm': [f'{algo_name} lower'] * qnt_train_sizes,\n",
    "            'hits': [algo_row_lower['hits'].values[0]] * qnt_train_sizes,\n",
    "            'hr': [algo_row_lower['hr'].values[0]] * qnt_train_sizes,\n",
    "            'time': [algo_row_lower['time'].values[0]] * qnt_train_sizes,\n",
    "            'train_size': train_sizes,\n",
    "            'test_size': [algo_row_lower['test_size'].values[0]] * qnt_train_sizes,\n",
    "            'test_interactions': [algo_row_lower['test_interactions'].values[0]] * qnt_train_sizes\n",
    "        })\n",
    "        df_results = pd.concat([df_results, df_lower])\n",
    "\n",
    "        algo_row_upper = df_results[(df_results['algorithm'] == algo_name) & (df_results['train_size'] == max_train_size)]\n",
    "        df_upper = pd.DataFrame({\n",
    "            'algorithm': [f'{algo_name} upper'] * qnt_train_sizes,\n",
    "            'hits': [algo_row_upper['hits'].values[0]] * qnt_train_sizes,\n",
    "            'hr': [algo_row_upper['hr'].values[0]] * qnt_train_sizes,\n",
    "            'time': [algo_row_upper['time'].values[0]] * qnt_train_sizes,\n",
    "            'train_size': train_sizes,\n",
    "            'test_size': [algo_row_upper['test_size'].values[0]] * qnt_train_sizes,\n",
    "            'test_interactions': [algo_row_upper['test_interactions'].values[0]] * qnt_train_sizes\n",
    "        })\n",
    "        df_results = pd.concat([df_results, df_upper])\n",
    "\n",
    "        df_results = df_results[df_results['algorithm'] != algo_name]\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_results(df_results, save_root):\n",
    "    df_results = remove_incremental(df_results)\n",
    "    df_results = transform_in_upper_and_lower_bounds(df_results, ['ALS', 'BPR'])\n",
    "\n",
    "    algos_configs = {\n",
    "        'ALS upper': {'color': 'blue', 'dash': 'dash'},\n",
    "        'ALS lower': {'color': 'blue', 'dash': 'dash'},\n",
    "        'BPR upper': {'color': 'red', 'dash': 'dash'},\n",
    "        'BPR lower': {'color': 'red', 'dash': 'dash'},\n",
    "        'LinUCB - ALS embeddings': {'color': 'green', 'dash': 'solid'},\n",
    "        'LinUCB - BPR embeddings': {'color': 'purple', 'dash': 'solid'},\n",
    "        'LinGreedy - ALS embeddings': {'color': 'orange', 'dash': 'solid'},\n",
    "        'LinGreedy - BPR embeddings': {'color': 'pink', 'dash': 'solid'}\n",
    "    }\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for algo_name, config in algos_configs.items():\n",
    "        df_algo = df_results[df_results['algorithm'] == algo_name]\n",
    "        fig.add_trace(go.Scatter(x=df_algo['train_size'], y=df_algo['hr'], mode='lines', name=algo_name, line=dict(color=config['color'], dash=config['dash'])))\n",
    "    \n",
    "    fig.update_layout(title='HR x Train size', xaxis_title='Train size', yaxis_title='HR')\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_html(f'{save_root}/hr_x_train_size.html')\n",
    "    fig.write_image(f'{save_root}/hr_x_train_size.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_results)\n",
    "\n",
    "plot_results(df_results, 'results-v14')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLRS-rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
